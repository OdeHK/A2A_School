# src/services/document_service.py
# Service trung t√¢m cho m·ªçi th·ª© li√™n quan ƒë·∫øn t√†i li·ªáu: x·ª≠ l√Ω, chunking, indexing, v√† RAG.

import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import uuid
import re

# S·ª≠ d·ª•ng relative import ƒë·ªÉ gi·ªØ c·∫•u tr√∫c module g·ªçn g√†ng v√† d·ªÖ b·∫£o tr√¨
from ..core.document_reader_optimized import OptimizedChapterDetector
from ..core.professional_pdf_processor import ProfessionalPDFProcessor
from ..core.vector_store import VectorStore
from ..core.data_structures import AgenticChunk, ChunkType, ChapterInfo
from ..db.database_manager import DatabaseManager

logger = logging.getLogger(__name__)

class DocumentService:
    """
    L√† "b·ªô n√£o" qu·∫£n l√Ω v√≤ng ƒë·ªùi c·ªßa m·ªôt t√†i li·ªáu, t·ª´ l√∫c upload cho ƒë·∫øn khi
    s·∫µn s√†ng ƒë·ªÉ h·ªèi ƒë√°p. Service n√†y ƒëi·ªÅu ph·ªëi ProfessionalPDFProcessor v√† VectorStore.
    """
    def __init__(
        self,
        config, # ƒê·ªëi t∆∞·ª£ng config ch·ª©a c√°c ƒë∆∞·ªùng d·∫´n v√† c√†i ƒë·∫∑t
        db_manager: DatabaseManager,
        vector_store: VectorStore,
        pdf_processor: ProfessionalPDFProcessor = None
    ):
        # √Åp d·ª•ng Dependency Injection: C√°c th√†nh ph·∫ßn ph·ª• thu·ªôc ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ b√™n ngo√†i.
        # ƒêi·ªÅu n√†y gi√∫p code linh ho·∫°t, d·ªÖ test v√† d·ªÖ thay th·∫ø c√°c th√†nh ph·∫ßn.
        self.config = config
        self.db_manager = db_manager
        self.vector_store = vector_store
        
        # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn ph·ª• thu·ªôc m√† service n√†y qu·∫£n l√Ω tr·ª±c ti·∫øp
        if pdf_processor:
            self.pdf_processor = pdf_processor
        else:
            # Fallback to professional PDF processor
            from ..core.llm_provider import LLMProvider
            fallback_llm = LLMProvider()
            self.pdf_processor = ProfessionalPDFProcessor(fallback_llm)

        # Tr·∫°ng th√°i trong b·ªô nh·ªõ ƒë·ªÉ truy c·∫≠p nhanh, tr√°nh g·ªçi DB li√™n t·ª•c cho c√°c t√°c v·ª• l·∫∑p l·∫°i
        self.processed_docs_cache: Dict[str, Dict] = {} 
        logger.info("‚úÖ DocumentService ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")

    def _convert_bookmarks_to_chapters(self, bookmarks: List[Dict]) -> List[Dict]:
        """
        Convert PDF bookmarks to chapter format expected by chunking logic.
        
        Args:
            bookmarks: List of bookmark dictionaries from ProfessionalPDFProcessor
            
        Returns:
            List of chapter dictionaries with number, title, and level
        """
        chapters = []
        for i, bookmark in enumerate(bookmarks):
            chapters.append({
                'number': i + 1,
                'title': bookmark.get('title', f'Chapter {i + 1}'),
                'level': bookmark.get('level', 1),
                'page': bookmark.get('page', 1),
                'start_position': 0  # Will be calculated during chunking
            })
        return chapters

    def process_document(self, file_path: Path) -> Optional[str]:
        """
        H√†m ch√≠nh ƒë·ªÉ x·ª≠ l√Ω m·ªôt file PDF m·ªõi m·ªôt c√°ch to√†n di·ªán.
        Bao g·ªìm ƒë·ªçc, chunking, v√† x√¢y d·ª±ng vector index.
        ƒê√¢y l√† m·ªôt t√°c v·ª• c√≥ th·ªÉ t·ªën th·ªùi gian, r·∫•t ph√π h·ª£p ƒë·ªÉ ch·∫°y trong n·ªÅn (background job).
        """
        # T·∫°o m·ªôt ID duy nh·∫•t cho t√†i li·ªáu
        doc_id = str(uuid.uuid4())
        filename = file_path.name
        
        try:
            logger.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t√†i li·ªáu '{filename}' v·ªõi ID: {doc_id}")
            # 1. Ghi nh·∫≠n v√†o DB l√† ƒëang x·ª≠ l√Ω ƒë·ªÉ UI c√≥ th·ªÉ theo d√µi tr·∫°ng th√°i
            self.db_manager.add_or_update_document(doc_id, filename, str(file_path), status='processing')
            
            # 2. Extract PDF structure using professional processor
            pdf_structure = self.pdf_processor.extract_pdf_structure(str(file_path))
            if 'error' in pdf_structure:
                raise ValueError(f"PDF processing failed: {pdf_structure['error']}")
            
            # Extract content for chunking
            full_text = pdf_structure.get('full_text', '')
            if not full_text:
                raise ValueError("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ t√†i li·ªáu.")
            
            # Create structured content compatible with existing chunking logic
            structured_content = {
                'full_text': full_text,
                'chapters': self._convert_bookmarks_to_chapters(pdf_structure.get('bookmarks', [])),
                'page_mapped_content': pdf_structure.get('page_mapped_content', {}),
                'structure_analysis': pdf_structure.get('structure_analysis', {})
            }

            # 3. Chia nh·ªè vƒÉn b·∫£n m·ªôt c√°ch th√¥ng minh (Agentic Chunking) v·ªõi page mapping
            chunks = self._create_agentic_chunks_with_pages(structured_content, filename, pdf_structure, doc_id)
            
            # 4. X√¢y d·ª±ng Vector Index cho t√†i li·ªáu n√†y th√¥ng qua VectorStore
            self.vector_store.build_index_for_doc(doc_id, chunks)
            
            # 5. C·∫≠p nh·∫≠t tr·∫°ng th√°i "ho√†n th√†nh" v√† th√¥ng tin chi ti·∫øt v√†o DB
            doc_info = {
                "id": doc_id,
                "filename": filename,
                "status": "completed",
                "chunks_count": len(chunks),
                "chapters": structured_content['chapters']
            }
            self.db_manager.add_or_update_document(
                doc_id, filename, str(file_path), 'completed', 
                len(chunks), structured_content['chapters']
            )
            
            # L∆∞u th√¥ng tin v√†o cache trong b·ªô nh·ªõ ƒë·ªÉ truy c·∫≠p nhanh ·ªü c√°c l·∫ßn g·ªçi sau
            self.processed_docs_cache[doc_id] = doc_info
            
            logger.info(f"üéâ X·ª≠ l√Ω th√†nh c√¥ng t√†i li·ªáu '{filename}' (ID: {doc_id})")
            return doc_id

        except Exception as e:
            logger.error(f"L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω t√†i li·ªáu '{filename}': {e}", exc_info=True)
            # C·∫≠p nh·∫≠t tr·∫°ng th√°i 'th·∫•t b·∫°i' v√†o DB
            self.db_manager.add_or_update_document(doc_id, filename, str(file_path), status='failed')
            return None
    def _create_agentic_chunks_with_pages(
        self, 
        structured_content: Dict, 
        filename: str, 
        pdf_structure: Dict,
        doc_id: str
    ) -> List[AgenticChunk]:
        """
        Create agentic chunks with accurate page-to-chapter mapping.
        
        Args:
            structured_content: Content structure with chapters and full text
            filename: Name of the source file
            pdf_structure: Complete PDF structure from professional processor
            doc_id: Document ID for chunk association
            
        Returns:
            List of AgenticChunk objects with accurate chapter associations
        """
        logger.info(f"üß© Creating page-mapped Agentic Chunks for file '{filename}'...")
        
        text = structured_content['full_text']
        chapters = structured_content['chapters']
        page_mapped_content = structured_content.get('page_mapped_content', {})
        structure_analysis = pdf_structure.get('structure_analysis', {})
        
        chunks_list = []
        
        # If we have page-mapped content and structure analysis, use page-based chunking
        if page_mapped_content and structure_analysis.get('page_ranges'):
            chunks_list = self._create_page_based_chunks(
                page_mapped_content, 
                structure_analysis['page_ranges'], 
                chapters,
                filename,
                doc_id
            )
        else:
            # Fallback to text-based chunking
            chunks_list = self._create_text_based_chunks(text, chapters, filename, doc_id)
        
        logger.info(f"‚úÖ Created {len(chunks_list)} agentic chunks with chapter mapping")
        return chunks_list

    def _create_page_based_chunks(self, page_mapped_content: Dict, page_ranges: Dict, chapters: List[Dict], filename: str, doc_id: str) -> List[AgenticChunk]:
        """
        Create chunks based on page-mapped content with proper chapter association.
        
        Args:
            page_mapped_content: Dictionary mapping page numbers to content
            page_ranges: Page ranges for different sections
            chapters: List of chapter information
            filename: Source filename
            doc_id: Document ID
            
        Returns:
            List of AgenticChunk objects with accurate page and chapter mapping
        """
        logger.info(f"üìÑ Creating page-based chunks for '{filename}'...")
        
        agentic_chunks = []
        chunk_position = 0
        
        # Process each page's content
        for page_num, page_content in page_mapped_content.items():
            if not page_content or not page_content.strip():
                continue
                
            # Find which chapter this page belongs to
            current_chapter_info = None
            for chapter in chapters:
                if chapter.get('page', 1) <= int(page_num):
                    current_chapter_info = ChapterInfo(
                        number=chapter.get('number'),
                        title=chapter.get('title')
                    )
            
            # Split page content into smaller chunks if needed
            page_text = page_content.strip()
            if len(page_text) > 800:  # Split large pages
                sentences = re.split(r'(?<=[.!?])\s+', page_text)
                current_chunk = ""
                
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) > 800 and current_chunk:
                        # Create chunk from accumulated sentences
                        agentic_chunks.append(AgenticChunk(
                            content=current_chunk.strip(),
                            chunk_type=ChunkType.PARAGRAPH,
                            source_file=filename,
                            position_in_document=chunk_position,
                            chapter_info=current_chapter_info,
                            page_number=int(page_num)
                        ))
                        chunk_position += 1
                        current_chunk = sentence
                    else:
                        current_chunk += " " + sentence
                
                # Add remaining content
                if current_chunk.strip():
                    agentic_chunks.append(AgenticChunk(
                        content=current_chunk.strip(),
                        chunk_type=ChunkType.PARAGRAPH,
                        source_file=filename,
                        position_in_document=chunk_position,
                        chapter_info=current_chapter_info,
                        page_number=int(page_num)
                    ))
                    chunk_position += 1
            else:
                # Small page, use as single chunk
                agentic_chunks.append(AgenticChunk(
                    content=page_text,
                    chunk_type=ChunkType.PARAGRAPH,
                    source_file=filename,
                    position_in_document=chunk_position,
                    chapter_info=current_chapter_info,
                    page_number=int(page_num)
                ))
                chunk_position += 1
        
        logger.info(f"‚úÖ Created {len(agentic_chunks)} page-based chunks")
        return agentic_chunks

    def _create_text_based_chunks(self, text: str, chapters: List[Dict], filename: str, doc_id: str) -> List[AgenticChunk]:
        """
        T·∫°o c√°c ƒë·ªëi t∆∞·ª£ng AgenticChunk t·ª´ n·ªôi dung ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch c·∫•u tr√∫c.
        M·ªói chunk kh√¥ng ch·ªâ l√† text m√† c√≤n ch·ª©a metadata quan tr·ªçng.
        """
        logger.info(f"üß© ƒêang t·∫°o Agentic Chunks cho file '{filename}'...")
        
        # Chia vƒÉn b·∫£n th√†nh c√°c c√¢u ƒë·ªÉ x·ª≠ l√Ω, gi√∫p chunk kh√¥ng b·ªã c·∫Øt gi·ªØa c√¢u
        sentences = re.split(r'(?<=[.!?])\s+', text.replace('\n', ' '))
        
        chunks_text = []
        current_chunk_content = ""
        chunk_size_target = 400 # K√≠ch th∆∞·ªõc m·ª•c ti√™u cho m·ªói chunk (t√≠nh b·∫±ng k√Ω t·ª±)

        for sentence in sentences:
            if len(current_chunk_content) + len(sentence) > chunk_size_target and current_chunk_content:
                chunks_text.append(current_chunk_content.strip())
                current_chunk_content = sentence
            else:
                current_chunk_content += " " + sentence

        if current_chunk_content:
            chunks_text.append(current_chunk_content.strip())

        agentic_chunks = []
        
        # Use page-based chapter mapping since we don't have line numbers
        page_to_chapter = {}
        for ch in chapters:
            page_to_chapter[ch.get('page', 1)] = ch
        
        # Sort pages for lookup
        sorted_pages = sorted(page_to_chapter.keys())

        for i, chunk_text in enumerate(chunks_text):
            current_chapter_info = None
            
            # For text-based chunks, we'll assign them to first chapter as fallback
            if chapters:
                # Find the most appropriate chapter (first available or by position)
                if len(chunks_text) > 1:
                    # Distribute chunks across chapters proportionally
                    chapter_index = min(int(i * len(chapters) / len(chunks_text)), len(chapters) - 1)
                    chapter = chapters[chapter_index]
                else:
                    # Single chunk gets first chapter
                    chapter = chapters[0]
                
                current_chapter_info = ChapterInfo(
                    number=chapter.get('number'),
                    title=chapter.get('title')
                )
            
            agentic_chunks.append(AgenticChunk(
                content=chunk_text,
                chunk_type=ChunkType.PARAGRAPH, # M·∫∑c ƒë·ªãnh l√† ƒëo·∫°n vƒÉn, c√≥ th·ªÉ m·ªü r·ªông ƒë·ªÉ ph√¢n lo·∫°i
                source_file=filename,
                position_in_document=i,
                chapter_info=current_chapter_info
            ))
        
        logger.info(f"ƒê√£ t·∫°o {len(agentic_chunks)} chunks.")
        return agentic_chunks

    def get_document(self, doc_id: str) -> Optional[Dict]:
        """L·∫•y th√¥ng tin m·ªôt t√†i li·ªáu (∆∞u ti√™n t·ª´ cache, sau ƒë√≥ m·ªõi truy v·∫•n DB)."""
        if doc_id in self.processed_docs_cache:
            return self.processed_docs_cache[doc_id]
        
        doc_from_db = self.db_manager.get_document(doc_id)
        if doc_from_db:
            self.processed_docs_cache[doc_id] = doc_from_db
        return doc_from_db

    def get_all_documents(self) -> List[Dict]:
        """L·∫•y danh s√°ch t√≥m t·∫Øt c·ªßa t·∫•t c·∫£ t√†i li·ªáu."""
        return self.db_manager.get_all_documents()

    def get_context_for_query(self, doc_id: str, query: str, chapter_filter: Optional[str] = None) -> str:
        """
        Th·ª±c hi·ªán RAG: t√¨m ki·∫øm v√† tr·∫£ v·ªÅ ng·ªØ c·∫£nh cho m·ªôt c√¢u h·ªèi c·ª• th·ªÉ,
        c√≥ th·ªÉ l·ªçc theo ch∆∞∆°ng.
        """
        logger.info(f"ƒêang th·ª±c hi·ªán RAG cho doc_id '{doc_id}' v·ªõi query: '{query[:50] if query else 'None'}...' v√† b·ªô l·ªçc ch∆∞∆°ng: '{chapter_filter}'")
        relevant_chunks = self.vector_store.search(query, top_k=5, chapter_filter=chapter_filter, doc_filter=doc_id)
        
        if not relevant_chunks:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu cho ph·∫°m vi ƒë√£ ch·ªçn."

        # N·ªëi c√°c chunk l·∫°i th√†nh m·ªôt ng·ªØ c·∫£nh duy nh·∫•t ƒë·ªÉ g·ª≠i cho LLM
        context = "\n\n---\n\n".join([
            f"[Tr√≠ch t·ª´ ch∆∞∆°ng: {chunk.chapter_info.title if chunk.chapter_info else 'Kh√¥ng r√µ'}]\n{chunk.content}"
            for chunk in relevant_chunks
        ])
        return context

