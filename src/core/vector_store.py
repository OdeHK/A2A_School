# src/core/vector_store.py
# Chá»‹u trÃ¡ch nhiá»‡m táº¡o vector embedding vÃ  quáº£n lÃ½ chá»‰ má»¥c tÃ¬m kiáº¿m FAISS.

import logging
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from typing import List, Tuple, Optional
from .data_structures import AgenticChunk

logger = logging.getLogger(__name__)

class VectorStore:
    """
    Quáº£n lÃ½ viá»‡c táº¡o embedding vÃ  tÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»“ng vá»›i FAISS.
    """
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.embedding_model: Optional[SentenceTransformer] = None
        self.index: Optional[faiss.Index] = None
        self.chunks: List[AgenticChunk] = [] # Giá»¯ tham chiáº¿u Ä‘áº¿n cÃ¡c chunk gá»‘c
        self._load_model()

    def _load_model(self):
        """Táº£i model embedding. Viá»‡c nÃ y chá»‰ thá»±c hiá»‡n má»™t láº§n."""
        try:
            if self.embedding_model is None:
                logger.info(f"ğŸ’¾ Äang táº£i model embedding: '{self.model_name}'... (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)")
                # trust_remote_code=True cáº§n thiáº¿t cho má»™t sá»‘ model trÃªn Hugging Face
                self.embedding_model = SentenceTransformer(self.model_name, trust_remote_code=True)
                logger.info("âœ… Model embedding Ä‘Ã£ táº£i xong.")
        except Exception as e:
            logger.error(f"Lá»—i nghiÃªm trá»ng khi táº£i model embedding: {e}", exc_info=True)
            raise

    def build_index(self, chunks: List[AgenticChunk]):
        """
        XÃ¢y dá»±ng hoáº·c cáº­p nháº­t chá»‰ má»¥c FAISS tá»« má»™t danh sÃ¡ch cÃ¡c chunk.
        """
        if not chunks:
            return
        
        logger.info(f"ğŸ› ï¸ Báº¯t Ä‘áº§u xÃ¢y dá»±ng/cáº­p nháº­t Vector Index cho {len(chunks)} chunks...")
        self.chunks.extend(chunks)
        
        # Chá»‰ táº¡o embedding cho cÃ¡c chunk má»›i
        contents_to_embed = [chunk.content for chunk in chunks]
        new_embeddings = self.embedding_model.encode(
            contents_to_embed,
            show_progress_bar=True,
            batch_size=32 # Xá»­ lÃ½ theo lÃ´ Ä‘á»ƒ hiá»‡u quáº£ hÆ¡n
        )

        # GÃ¡n láº¡i embedding vÃ o Ä‘á»‘i tÆ°á»£ng chunk
        for chunk, embedding in zip(chunks, new_embeddings):
            chunk.embedding = embedding

        # Cáº­p nháº­t chá»‰ má»¥c FAISS
        if self.index is None:
            dimension = new_embeddings.shape[1]
            # IndexFlatIP hiá»‡u quáº£ cho tÃ¬m kiáº¿m cosine similarity sau khi Ä‘Ã£ chuáº©n hÃ³a vector
            self.index = faiss.IndexFlatIP(dimension)
        
        # Chuáº©n hÃ³a vector (L2 normalization) Ä‘á»ƒ cosine similarity tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i inner product
        faiss.normalize_L2(new_embeddings)
        self.index.add(new_embeddings.astype('float32'))
        
        logger.info(f"âœ… Vector Index Ä‘Æ°á»£c cáº­p nháº­t. Tá»•ng sá»‘ vectors: {self.index.ntotal}")

    def search(self, query: str, top_k: int = 5, chapter_filter: Optional[str] = None, doc_filter: Optional[str] = None) -> List[AgenticChunk]:
        """
        TÃ¬m kiáº¿m cÃ¡c chunk liÃªn quan nháº¥t Ä‘áº¿n cÃ¢u há»i.
        CÃ³ há»— trá»£ lá»c theo chÆ°Æ¡ng.
        """
        if self.index is None or not self.chunks:
            logger.warning(f"VectorStore chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o hoáº·c khÃ´ng cÃ³ chunks. Index: {self.index is not None}, Chunks: {len(self.chunks) if self.chunks else 0}")
            return []

        logger.info(f"ğŸ” Thá»±c hiá»‡n tÃ¬m kiáº¿m vector cho cÃ¢u há»i: '{query[:50]}...' trong {len(self.chunks)} chunks")
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k * 2) # Láº¥y nhiá»u hÆ¡n Ä‘á»ƒ lá»c

        results = []
        for score, idx in zip(scores[0], indices[0]):
            # NgÆ°á»¡ng tÆ°Æ¡ng Ä‘á»“ng Ä‘á»ƒ loáº¡i bá» káº¿t quáº£ nhiá»…u (giáº£m xuá»‘ng Ä‘á»ƒ tÃ¬m Ä‘Æ°á»£c nhiá»u káº¿t quáº£ hÆ¡n)
            if score < 0.3: 
                continue

            chunk = self.chunks[idx]
            
            # Kiá»ƒm tra bá»™ lá»c tÃ i liá»‡u trÆ°á»›c (náº¿u cÃ³)
            if doc_filter:
                # Táº¡m thá»i bá» qua filter theo doc_id vÃ¬ cáº§n map vá»›i source_file
                # Sáº½ implement sau khi cÃ³ mapping logic
                pass
            
            # Ãp dá»¥ng bá»™ lá»c chÆ°Æ¡ng náº¿u cÃ³
            if chapter_filter:
                if chunk.chapter_info and chapter_filter.lower() in chunk.chapter_info.title.lower():
                    results.append(chunk)
            else:
                # Náº¿u khÃ´ng cÃ³ filter chÆ°Æ¡ng, thÃªm chunk vÃ o káº¿t quáº£
                results.append(chunk)
        
        logger.info(f"TÃ¬m tháº¥y {len(results)} chunks liÃªn quan (sau khi lá»c).")
        return results[:top_k]

    def build_index_for_doc(self, doc_id: str, chunks: List[AgenticChunk]) -> None:
        """
        XÃ¢y dá»±ng index cho má»™t tÃ i liá»‡u cá»¥ thá»ƒ.
        """
        logger.info(f"Äang xÃ¢y dá»±ng vector index cho tÃ i liá»‡u {doc_id} vá»›i {len(chunks)} chunks...")
        
        if not chunks:
            logger.warning(f"KhÃ´ng cÃ³ chunks nÃ o Ä‘á»ƒ index cho tÃ i liá»‡u {doc_id}")
            return
        
        # ThÃªm chunks vÃ o danh sÃ¡ch hiá»‡n táº¡i
        self.chunks.extend(chunks)
        
        # Táº¡o embeddings cho chunks má»›i
        texts = [chunk.content for chunk in chunks]
        embeddings = self.embedding_model.encode(texts)
        
        # Chuáº©n hÃ³a embeddings
        faiss.normalize_L2(embeddings)
        
        # ThÃªm vÃ o FAISS index
        if self.index is None:
            # Táº¡o index má»›i náº¿u chÆ°a cÃ³
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
        
        self.index.add(embeddings.astype('float32'))
        
        # Cáº­p nháº­t embeddings cho chunks
        start_idx = len(self.chunks) - len(chunks)
        for i, chunk in enumerate(chunks):
            chunk.embedding = embeddings[i]
        
        logger.info(f"âœ… ÄÃ£ thÃªm {len(chunks)} chunks vÃ o vector store. Tá»•ng: {len(self.chunks)} chunks.")

    def get_chunks_for_document(self, doc_id: str) -> List[AgenticChunk]:
        """
        Láº¥y táº¥t cáº£ chunks thuá»™c vá» má»™t document cá»¥ thá»ƒ.
        """
        # Filter chunks by source file (assuming filename contains doc info)
        # This is a simple implementation - in production, you might want to store doc_id in chunks
        doc_chunks = []
        for chunk in self.chunks:
            # Check if chunk belongs to this document
            # This is a simple heuristic - you might want to improve this
            if hasattr(chunk, 'source_file') and chunk.source_file:
                doc_chunks.append(chunk)
        
        logger.info(f"Retrieved {len(doc_chunks)} chunks for document {doc_id}")
        return doc_chunks

    def get_chunks_by_doc_id(self, doc_id: str) -> List[AgenticChunk]:
        """
        Get all chunks belonging to a specific document ID.
        
        Args:
            doc_id: Document identifier
            
        Returns:
            List of AgenticChunk objects for the document
        """
        doc_chunks = []
        for chunk in self.chunks:
            # Check if chunk has source_info with document reference
            if (hasattr(chunk, 'source_info') and 
                chunk.source_info and 
                'doc_id' in chunk.source_info and 
                chunk.source_info['doc_id'] == doc_id):
                doc_chunks.append(chunk)
            # Fallback: check if chunk has any reference to this doc_id
            elif hasattr(chunk, 'id') and doc_id in str(chunk.id):
                doc_chunks.append(chunk)
        
        logger.info(f"ğŸ“„ Retrieved {len(doc_chunks)} chunks for document {doc_id}")
        return doc_chunks

    def get_chunks_by_chapter(self, doc_id: str, chapter_title: str) -> List[AgenticChunk]:
        """
        Get all chunks belonging to a specific chapter in a document.
        
        Args:
            doc_id: Document identifier
            chapter_title: Title of the chapter
            
        Returns:
            List of AgenticChunk objects for the specified chapter
        """
        chapter_chunks = []
        doc_chunks = self.get_chunks_by_doc_id(doc_id)
        
        for chunk in doc_chunks:
            # Check if chunk belongs to the specified chapter
            if (chunk.chapter_info and 
                chunk.chapter_info.title and 
                chunk.chapter_info.title.strip().lower() == chapter_title.strip().lower()):
                chapter_chunks.append(chunk)
        
        logger.info(f"ğŸ“š Retrieved {len(chapter_chunks)} chunks for chapter '{chapter_title}' in document {doc_id}")
        return chapter_chunks

    def get_document_chapters(self, doc_id: str) -> List[str]:
        """
        Get list of unique chapter titles in a document.
        
        Args:
            doc_id: Document identifier
            
        Returns:
            List of chapter titles
        """
        chapters = set()
        doc_chunks = self.get_chunks_by_doc_id(doc_id)
        
        for chunk in doc_chunks:
            if chunk.chapter_info and chunk.chapter_info.title:
                chapters.add(chunk.chapter_info.title)
        
        chapter_list = sorted(list(chapters))
        logger.info(f"ğŸ“‘ Found {len(chapter_list)} chapters in document {doc_id}")
        return chapter_list
