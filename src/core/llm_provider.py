# src/core/llm_provider.py
# L·ªõp giao ti·∫øp v·ªõi API c·ªßa OpenRouter, ƒë∆∞·ª£c t·ªëi ∆∞u v·ªõi caching v√† rate limiting.

import requests
import logging
import time
import json
import hashlib
from pathlib import Path
from collections import deque
from typing import Dict, Optional

logger = logging.getLogger(__name__)

class RateLimiter:
    """
    ƒêi·ªÅu ti·∫øt t·∫ßn su·∫•t g·ªçi API ƒë·ªÉ tr√°nh b·ªã kh√≥a.
    """
    def __init__(self, requests_per_minute: int = 20):
        self.requests_per_minute = requests_per_minute
        self.request_times = deque()

    def wait_if_needed(self):
        """ƒê·ª£i n·∫øu c·∫ßn thi·∫øt tr∆∞·ªõc khi th·ª±c hi·ªán request ti·∫øp theo."""
        now = time.time()
        # X√≥a c√°c request ƒë√£ qu√° 1 ph√∫t
        while self.request_times and now - self.request_times[0] > 60:
            self.request_times.popleft()
        
        if len(self.request_times) >= self.requests_per_minute:
            wait_time = 60 - (now - self.request_times[0])
            if wait_time > 0:
                logger.info(f"‚è≥ Rate limit ƒë·∫°t ng∆∞·ª°ng, ch·ªù {wait_time:.1f} gi√¢y...")
                time.sleep(wait_time)
        
        self.request_times.append(time.time())

class LLMProvider:
    """
    L·ªõp ch√≠nh ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi LLM, t√≠ch h·ª£p caching v√† rate limiting.
    """
    def __init__(self, api_key: str, api_url: str, model: str, cache_dir: Path):
        self.api_key = api_key
        self.api_url = api_url
        self.model = model
        self.rate_limiter = RateLimiter()
        
        # C·∫•u h√¨nh caching
        self.cache_dir = cache_dir / "llm_cache"
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache: Dict[str, str] = {}

        # Ch·∫ø ƒë·ªô mock n·∫øu kh√¥ng c√≥ API key
        self.is_mock = not api_key or "your_" in api_key
        if self.is_mock:
            logger.warning("LLMProvider ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô MOCK do kh√¥ng c√≥ API key.")
        else:
            logger.info(f"‚úÖ LLMProvider ƒë√£ kh·ªüi t·∫°o v·ªõi model: {self.model}")

    def _get_cache_key(self, system_prompt: str, user_prompt: str) -> str:
        """T·∫°o m·ªôt key duy nh·∫•t cho request ƒë·ªÉ caching."""
        content = f"model:{self.model}|sys:{system_prompt}|user:{user_prompt}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_from_cache(self, key: str) -> Optional[str]:
        """L·∫•y k·∫øt qu·∫£ t·ª´ cache (memory tr∆∞·ªõc, sau ƒë√≥ ƒë·∫øn disk)."""
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        cache_file = self.cache_dir / f"{key}.json"
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.memory_cache[key] = data['response'] # C·∫≠p nh·∫≠t memory cache
                return data['response']
        return None

    def _save_to_cache(self, key: str, response: str):
        """L∆∞u k·∫øt qu·∫£ v√†o c·∫£ memory v√† disk cache."""
        self.memory_cache[key] = response
        cache_file = self.cache_dir / f"{key}.json"
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({'response': response}, f, ensure_ascii=False, indent=2)

    def generate(self, system_prompt: str, user_prompt: str, max_tokens: int = 2048) -> str:
        """
        Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ sinh vƒÉn b·∫£n.
        T√≠ch h·ª£p s·∫µn mock mode, caching, rate limiting v√† retry.
        """
        if self.is_mock:
            return f"**[CH·∫æ ƒê·ªò MOCK]**\n**System Prompt:** {system_prompt}\n**User Prompt:** {user_prompt[:200]}..."

        cache_key = self._get_cache_key(system_prompt, user_prompt)
        cached_response = self._get_from_cache(cache_key)
        if cached_response:
            logger.info(f"üéØ Cache hit cho prompt: '{user_prompt[:50]}...'")
            return cached_response
        
        logger.info(f"üåê Cache miss, ƒëang g·ªçi API cho prompt: '{user_prompt[:50]}...'")
        self.rate_limiter.wait_if_needed()

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "http://localhost", # C·∫ßn thi·∫øt cho OpenRouter
            "X-Title": "A2A School Platform"
        }
        json_data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": max_tokens
        }

        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(self.api_url, headers=headers, json=json_data, timeout=90)
                response.raise_for_status()
                result_json = response.json()
                content = result_json['choices'][0]['message']['content']
                self._save_to_cache(cache_key, content)
                return content
            except requests.exceptions.RequestException as e:
                logger.warning(f"L·ªói API (l·∫ßn {attempt + 1}/{max_retries}): {e}. ƒêang th·ª≠ l·∫°i...")
                time.sleep(2 ** attempt) # Exponential backoff
        
        logger.error(f"Kh√¥ng th·ªÉ ho√†n th√†nh request API sau {max_retries} l·∫ßn th·ª≠.")
        return "[L·ªñI] Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn LLM. Vui l√≤ng th·ª≠ l·∫°i sau."
